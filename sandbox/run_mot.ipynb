{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" run_mot_challenge.py\n",
    "\n",
    "Run example:\n",
    "run_mot_challenge.py --USE_PARALLEL False --METRICS Hota --TRACKERS_TO_EVAL Lif_T\n",
    "\n",
    "Command Line Arguments: Defaults, # Comments\n",
    "    Eval arguments:\n",
    "        'USE_PARALLEL': False,\n",
    "        'NUM_PARALLEL_CORES': 8,\n",
    "        'BREAK_ON_ERROR': True,\n",
    "        'PRINT_RESULTS': True,\n",
    "        'PRINT_ONLY_COMBINED': False,\n",
    "        'PRINT_CONFIG': True,\n",
    "        'TIME_PROGRESS': True,\n",
    "        'OUTPUT_SUMMARY': True,\n",
    "        'OUTPUT_DETAILED': True,\n",
    "        'PLOT_CURVES': True,\n",
    "    Dataset arguments:\n",
    "        'GT_FOLDER': os.path.join(code_path, 'data/gt/mot_challenge/'),  # Location of GT data\n",
    "        'TRACKERS_FOLDER': os.path.join(code_path, 'data/trackers/mot_challenge/'),  # Trackers location\n",
    "        'OUTPUT_FOLDER': None,  # Where to save eval results (if None, same as TRACKERS_FOLDER)\n",
    "        'TRACKERS_TO_EVAL': None,  # Filenames of trackers to eval (if None, all in folder)\n",
    "        'CLASSES_TO_EVAL': ['pedestrian'],  # Valid: ['pedestrian']\n",
    "        'BENCHMARK': 'MOT17',  # Valid: 'MOT17', 'MOT16', 'MOT20', 'MOT15'\n",
    "        'SPLIT_TO_EVAL': 'train',  # Valid: 'train', 'test', 'all'\n",
    "        'INPUT_AS_ZIP': False,  # Whether tracker input files are zipped\n",
    "        'PRINT_CONFIG': True,  # Whether to print current config\n",
    "        'DO_PREPROC': True,  # Whether to perform preprocessing (never done for 2D_MOT_2015)\n",
    "        'TRACKER_SUB_FOLDER': 'data',  # Tracker files are in TRACKER_FOLDER/tracker_name/TRACKER_SUB_FOLDER\n",
    "        'OUTPUT_SUB_FOLDER': '',  # Output files are saved in OUTPUT_FOLDER/tracker_name/OUTPUT_SUB_FOLDER\n",
    "    Metric arguments:\n",
    "        'METRICS': ['HOTA', 'CLEAR', 'Identity', 'VACE']\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "from multiprocessing import freeze_support\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sys.path.append('../')\n",
    "import trackeval  # noqa: E402\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line interface:\n",
    "default_eval_config = trackeval.Evaluator.get_default_eval_config()\n",
    "default_eval_config['DISPLAY_LESS_PROGRESS'] = False\n",
    "default_dataset_config = trackeval.datasets.MotChallenge2DBox.get_default_dataset_config()\n",
    "default_metrics_config = {'METRICS': ['HOTA', 'CLEAR', 'Identity'], 'THRESHOLD': 0.5}\n",
    "config = {**default_eval_config, **default_dataset_config, **default_metrics_config}  # Merge default configs\n",
    "parser = argparse.ArgumentParser()\n",
    "for setting in config.keys():\n",
    "    if type(config[setting]) == list or type(config[setting]) == type(None):\n",
    "        parser.add_argument(\"--\" + setting, nargs='+')\n",
    "    else:\n",
    "        parser.add_argument(\"--\" + setting)\n",
    "args = parser.parse_args(args=[]).__dict__\n",
    "for setting in args.keys():\n",
    "    if args[setting] is not None:\n",
    "        if type(config[setting]) == type(True):\n",
    "            if args[setting] == 'True':\n",
    "                x = True\n",
    "            elif args[setting] == 'False':\n",
    "                x = False\n",
    "            else:\n",
    "                raise Exception('Command line parameter ' + setting + 'must be True or False')\n",
    "        elif type(config[setting]) == type(1):\n",
    "            x = int(args[setting])\n",
    "        elif type(args[setting]) == type(None):\n",
    "            x = None\n",
    "        elif setting == 'SEQ_INFO':\n",
    "            x = dict(zip(args[setting], [None]*len(args[setting])))\n",
    "        else:\n",
    "            x = args[setting]\n",
    "        config[setting] = x\n",
    "eval_config = {k: v for k, v in config.items() if k in default_eval_config.keys()}\n",
    "dataset_config = {k: v for k, v in config.items() if k in default_dataset_config.keys()}\n",
    "metrics_config = {k: v for k, v in config.items() if k in default_metrics_config.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval Config:\n",
      "USE_PARALLEL         : False                         \n",
      "NUM_PARALLEL_CORES   : 8                             \n",
      "BREAK_ON_ERROR       : True                          \n",
      "RETURN_ON_ERROR      : False                         \n",
      "LOG_ON_ERROR         : /home/guest/dev_repo/MOT_eval/error_log.txt\n",
      "PRINT_RESULTS        : True                          \n",
      "PRINT_ONLY_COMBINED  : False                         \n",
      "PRINT_CONFIG         : True                          \n",
      "TIME_PROGRESS        : True                          \n",
      "DISPLAY_LESS_PROGRESS : False                         \n",
      "OUTPUT_SUMMARY       : True                          \n",
      "OUTPUT_EMPTY_CLASSES : True                          \n",
      "OUTPUT_DETAILED      : True                          \n",
      "PLOT_CURVES          : True                          \n",
      "\n",
      "MotChallenge2DBox Config:\n",
      "PRINT_CONFIG         : True                          \n",
      "GT_FOLDER            : /home/guest/dev_repo/MOT_eval/data/gt/mot_challenge/\n",
      "TRACKERS_FOLDER      : /home/guest/dev_repo/MOT_eval/data/trackers/mot_challenge/\n",
      "OUTPUT_FOLDER        : None                          \n",
      "TRACKERS_TO_EVAL     : None                          \n",
      "CLASSES_TO_EVAL      : ['pedestrian']                \n",
      "BENCHMARK            : MOT17                         \n",
      "SPLIT_TO_EVAL        : train                         \n",
      "INPUT_AS_ZIP         : False                         \n",
      "DO_PREPROC           : True                          \n",
      "TRACKER_SUB_FOLDER   : data                          \n",
      "OUTPUT_SUB_FOLDER    :                               \n",
      "TRACKER_DISPLAY_NAMES : None                          \n",
      "SEQMAP_FOLDER        : None                          \n",
      "SEQMAP_FILE          : None                          \n",
      "SEQ_INFO             : None                          \n",
      "GT_LOC_FORMAT        : {gt_folder}/{seq}/gt/gt.txt   \n",
      "SKIP_SPLIT_FOL       : False                         \n",
      "\n",
      "CLEAR Config:\n",
      "METRICS              : ['HOTA', 'CLEAR', 'Identity'] \n",
      "THRESHOLD            : 0.5                           \n",
      "PRINT_CONFIG         : True                          \n",
      "\n",
      "Identity Config:\n",
      "METRICS              : ['HOTA', 'CLEAR', 'Identity'] \n",
      "THRESHOLD            : 0.5                           \n",
      "PRINT_CONFIG         : True                          \n"
     ]
    }
   ],
   "source": [
    "# Run code\n",
    "evaluator = trackeval.Evaluator(eval_config)\n",
    "dataset_list = [trackeval.datasets.MotChallenge2DBox(dataset_config)]\n",
    "metrics_list = []\n",
    "for metric in [trackeval.metrics.HOTA, trackeval.metrics.CLEAR, trackeval.metrics.Identity, trackeval.metrics.VACE]:\n",
    "    if metric.get_name() in metrics_config['METRICS']:\n",
    "        metrics_list.append(metric(metrics_config))\n",
    "if len(metrics_list) == 0:\n",
    "    raise Exception('No metrics selected for evaluation')\n",
    "# evaluator.evaluate(dataset_list, metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tracker_list': ['MPNTrack'],\n",
       " 'seq_list': ['MOT17-02-DPM',\n",
       "  'MOT17-04-DPM',\n",
       "  'MOT17-05-DPM',\n",
       "  'MOT17-09-DPM',\n",
       "  'MOT17-10-DPM',\n",
       "  'MOT17-11-DPM',\n",
       "  'MOT17-13-DPM',\n",
       "  'MOT17-02-FRCNN',\n",
       "  'MOT17-04-FRCNN',\n",
       "  'MOT17-05-FRCNN',\n",
       "  'MOT17-09-FRCNN',\n",
       "  'MOT17-10-FRCNN',\n",
       "  'MOT17-11-FRCNN',\n",
       "  'MOT17-13-FRCNN',\n",
       "  'MOT17-02-SDP',\n",
       "  'MOT17-04-SDP',\n",
       "  'MOT17-05-SDP',\n",
       "  'MOT17-09-SDP',\n",
       "  'MOT17-10-SDP',\n",
       "  'MOT17-11-SDP',\n",
       "  'MOT17-13-SDP'],\n",
       " 'class_list': ['pedestrian'],\n",
       " 'output_fol': '/home/guest/dev_repo/MOT_eval/data/trackers/mot_challenge/MOT17-train',\n",
       " 'output_sub_fol': '',\n",
       " 'should_classes_combine': False,\n",
       " 'use_super_categories': False,\n",
       " 'config': {'PRINT_CONFIG': True,\n",
       "  'GT_FOLDER': '/home/guest/dev_repo/MOT_eval/data/gt/mot_challenge/',\n",
       "  'TRACKERS_FOLDER': '/home/guest/dev_repo/MOT_eval/data/trackers/mot_challenge/',\n",
       "  'OUTPUT_FOLDER': None,\n",
       "  'TRACKERS_TO_EVAL': None,\n",
       "  'CLASSES_TO_EVAL': ['pedestrian'],\n",
       "  'BENCHMARK': 'MOT17',\n",
       "  'SPLIT_TO_EVAL': 'train',\n",
       "  'INPUT_AS_ZIP': False,\n",
       "  'DO_PREPROC': True,\n",
       "  'TRACKER_SUB_FOLDER': 'data',\n",
       "  'OUTPUT_SUB_FOLDER': '',\n",
       "  'TRACKER_DISPLAY_NAMES': None,\n",
       "  'SEQMAP_FOLDER': None,\n",
       "  'SEQMAP_FILE': None,\n",
       "  'SEQ_INFO': None,\n",
       "  'GT_LOC_FORMAT': '{gt_folder}/{seq}/gt/gt.txt',\n",
       "  'SKIP_SPLIT_FOL': False},\n",
       " 'benchmark': 'MOT17',\n",
       " 'gt_set': 'MOT17-train',\n",
       " 'gt_fol': '/home/guest/dev_repo/MOT_eval/data/gt/mot_challenge/MOT17-train',\n",
       " 'tracker_fol': '/home/guest/dev_repo/MOT_eval/data/trackers/mot_challenge/MOT17-train',\n",
       " 'data_is_zipped': False,\n",
       " 'do_preproc': True,\n",
       " 'tracker_sub_fol': 'data',\n",
       " 'valid_classes': ['pedestrian'],\n",
       " 'class_name_to_class_id': {'pedestrian': 1,\n",
       "  'person_on_vehicle': 2,\n",
       "  'car': 3,\n",
       "  'bicycle': 4,\n",
       "  'motorbike': 5,\n",
       "  'non_mot_vehicle': 6,\n",
       "  'static_person': 7,\n",
       "  'distractor': 8,\n",
       "  'occluder': 9,\n",
       "  'occluder_on_ground': 10,\n",
       "  'occluder_full': 11,\n",
       "  'reflection': 12,\n",
       "  'crowd': 13},\n",
       " 'valid_class_numbers': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
       " 'seq_lengths': {'MOT17-02-DPM': 600,\n",
       "  'MOT17-04-DPM': 1050,\n",
       "  'MOT17-05-DPM': 837,\n",
       "  'MOT17-09-DPM': 525,\n",
       "  'MOT17-10-DPM': 654,\n",
       "  'MOT17-11-DPM': 900,\n",
       "  'MOT17-13-DPM': 750,\n",
       "  'MOT17-02-FRCNN': 600,\n",
       "  'MOT17-04-FRCNN': 1050,\n",
       "  'MOT17-05-FRCNN': 837,\n",
       "  'MOT17-09-FRCNN': 525,\n",
       "  'MOT17-10-FRCNN': 654,\n",
       "  'MOT17-11-FRCNN': 900,\n",
       "  'MOT17-13-FRCNN': 750,\n",
       "  'MOT17-02-SDP': 600,\n",
       "  'MOT17-04-SDP': 1050,\n",
       "  'MOT17-05-SDP': 837,\n",
       "  'MOT17-09-SDP': 525,\n",
       "  'MOT17-10-SDP': 654,\n",
       "  'MOT17-11-SDP': 900,\n",
       "  'MOT17-13-SDP': 750},\n",
       " 'tracker_to_disp': {'MPNTrack': 'MPNTrack'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "from multiprocessing.pool import Pool\n",
    "from functools import partial\n",
    "import os\n",
    "from trackeval import utils\n",
    "from trackeval.utils import TrackEvalException\n",
    "from trackeval import _timing\n",
    "from trackeval.metrics import Count\n",
    "\n",
    "def evaluate(dataset_list, metrics_list):\n",
    "    \"\"\"Evaluate a set of metrics on a set of datasets\"\"\"\n",
    "    # config = config\n",
    "    metrics_list = metrics_list + [Count()]  # Count metrics are always run\n",
    "    metric_names = utils.validate_metrics_list(metrics_list)\n",
    "    dataset_names = [dataset.get_name() for dataset in dataset_list]\n",
    "    print(dataset_names)\n",
    "    output_res = {}\n",
    "    output_msg = {}\n",
    "\n",
    "    for dataset, dataset_name in zip(dataset_list, dataset_names):\n",
    "        # Get dataset info about what to evaluate\n",
    "        output_res[dataset_name] = {}\n",
    "        print('datast',dataset)\n",
    "        output_msg[dataset_name] = {}\n",
    "        tracker_list, seq_list, class_list = dataset.get_eval_info()\n",
    "        print(tracker_list)\n",
    "        print('\\nEvaluating %i tracker(s) on %i sequence(s) for %i class(es) on %s dataset using the following '\n",
    "                'metrics: %s\\n' % (len(tracker_list), len(seq_list), len(class_list), dataset_name,\n",
    "                                    ', '.join(metric_names)))\n",
    "        \n",
    "        # Evaluate each tracker\n",
    "        for tracker in tracker_list:\n",
    "        # if not config['BREAK_ON_ERROR'] then go to next tracker without breaking\n",
    "            try:\n",
    "                # Evaluate each sequence in parallel or in series.\n",
    "                # returns a nested dict (res), indexed like: res[seq][class][metric_name][sub_metric field]\n",
    "                # e.g. res[seq_0001][pedestrian][hota][DetA]\n",
    "                print('\\nEvaluating %s\\n' % tracker)\n",
    "                time_start = time.time()\n",
    "                if config['USE_PARALLEL']:\n",
    "                    with Pool(config['NUM_PARALLEL_CORES']) as pool:\n",
    "                        _eval_sequence = partial(eval_sequence, dataset=dataset, tracker=tracker,\n",
    "                                                    class_list=class_list, metrics_list=metrics_list,\n",
    "                                                    metric_names=metric_names)\n",
    "                        results = pool.map(_eval_sequence, seq_list)\n",
    "                        res = dict(zip(seq_list, results))\n",
    "\n",
    "                else:\n",
    "                    res = {}\n",
    "                    for curr_seq in sorted(seq_list):\n",
    "                        res[curr_seq] = eval_sequence(curr_seq, dataset, tracker, class_list, metrics_list,\n",
    "                                                        metric_names)     \n",
    "                        # print(res) \n",
    "                    return res                                                                    \n",
    "                \n",
    "            except Exception as err:\n",
    "                continue\n",
    "\n",
    "# @_timing.time\n",
    "def eval_sequence(seq, dataset, tracker, class_list, metrics_list, metric_names):\n",
    "    \"\"\"Function for evaluating a single sequence\"\"\"\n",
    "\n",
    "    raw_data = dataset.get_raw_seq_data(tracker, seq) ##ここにbbox_dfとかが入ってほしい。\n",
    "    seq_res = {}\n",
    "    # print(class_list)\n",
    "    for cls in class_list:\n",
    "        seq_res[cls] = {}\n",
    "        # print(seq_res)\n",
    "        data = dataset.get_preprocessed_seq_data(raw_data, cls)\n",
    "        # print(data.keys())\n",
    "        for metric, met_name in zip(metrics_list, metric_names):\n",
    "            seq_res[cls][met_name] = metric.eval_sequence(data)\n",
    "    return seq_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MotChallenge2DBox']\n",
      "datast <trackeval.datasets.mot_challenge_2d_box.MotChallenge2DBox object at 0x7f0e5764ef40>\n",
      "['MPNTrack']\n",
      "\n",
      "Evaluating 1 tracker(s) on 21 sequence(s) for 1 class(es) on MotChallenge2DBox dataset using the following metrics: HOTA, CLEAR, Identity, Count\n",
      "\n",
      "\n",
      "Evaluating MPNTrack\n",
      "\n",
      "    MotChallenge2DBox.get_raw_seq_data(MPNTrack, MOT17-02-DPM)             0.2384 sec\n",
      "    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.2103 sec\n",
      "    HOTA.eval_sequence()                                                   0.2276 sec\n",
      "    CLEAR.eval_sequence()                                                  0.0401 sec\n",
      "    Identity.eval_sequence()                                               0.0092 sec\n",
      "    Count.eval_sequence()                                                  0.0000 sec\n",
      "    MotChallenge2DBox.get_raw_seq_data(MPNTrack, MOT17-02-FRCNN)           0.2166 sec\n",
      "    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.1889 sec\n",
      "    HOTA.eval_sequence()                                                   0.2270 sec\n",
      "    CLEAR.eval_sequence()                                                  0.0414 sec\n",
      "    Identity.eval_sequence()                                               0.0089 sec\n",
      "    Count.eval_sequence()                                                  0.0000 sec\n",
      "    MotChallenge2DBox.get_raw_seq_data(MPNTrack, MOT17-02-SDP)             0.2881 sec\n",
      "    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.1897 sec\n",
      "    HOTA.eval_sequence()                                                   0.2594 sec\n",
      "    CLEAR.eval_sequence()                                                  0.0433 sec\n",
      "    Identity.eval_sequence()                                               0.0189 sec\n",
      "    Count.eval_sequence()                                                  0.0000 sec\n",
      "    MotChallenge2DBox.get_raw_seq_data(MPNTrack, MOT17-04-DPM)             0.9007 sec\n",
      "    MotChallenge2DBox.get_preprocessed_seq_data(pedestrian)                0.3997 sec\n",
      "    HOTA.eval_sequence()                                                   0.4581 sec\n",
      "    CLEAR.eval_sequence()                                                  0.0927 sec\n",
      "    Identity.eval_sequence()                                               0.0201 sec\n",
      "    Count.eval_sequence()                                                  0.0000 sec\n",
      "    MotChallenge2DBox.get_raw_seq_data(MPNTrack, MOT17-04-FRCNN)           0.9591 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16719/3443563197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_16719/67444100.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset_list, metrics_list)\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mcurr_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                         res[curr_seq] = eval_sequence(curr_seq, dataset, tracker, class_list, metrics_list,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                                         metric_names)     \n\u001b[1;32m     54\u001b[0m                         \u001b[0;31m# print(res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_16719/67444100.py\u001b[0m in \u001b[0;36meval_sequence\u001b[0;34m(seq, dataset, tracker, class_list, metrics_list, metric_names)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mseq_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# print(seq_res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preprocessed_seq_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;31m# print(data.keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmet_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev_repo/MOT_eval/sandbox/../trackeval/_timing.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Run function with timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev_repo/MOT_eval/sandbox/../trackeval/datasets/mot_challenge_2d_box.py\u001b[0m in \u001b[0;36mget_preprocessed_seq_data\u001b[0;34m(self, raw_data, cls)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mmatching_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mmatching_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatching_scores\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m                 \u001b[0mmatch_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_sum_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmatching_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m                 \u001b[0mactually_matched_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatching_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0mmatch_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactually_matched_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/base_atom/lib/python3.8/site-packages/scipy/optimize/_lsap.py\u001b[0m in \u001b[0;36mlinear_sum_assignment\u001b[0;34m(cost_matrix, maximize)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mcost_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcost_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lsap_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate(dataset_list, metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base_atom')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "47bcc98a91e0639070087bf2bbd0f353a7498108652c8107efb5221696e92166"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
